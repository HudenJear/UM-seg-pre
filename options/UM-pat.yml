# general settings
name: UM-APT
model_type: AuxModel
num_gpu: 1  # set num_gpu: 0 for cpu mode
# manual_seed: 114514
multitrain: true
round: 1
dataset_random_fetch: false
# random_fetch_path: dataset/slo_label.csv
# val_ratio: 0.05
# if dataset_random_fetch is on, the csv_path would be disbaled in the train and val phase.
# a new csv would be created and saved in the models folder


# dataset and data loader settings
datasets:
  train:
    name: UMsegTrain
    type: ImagePairDataset
    image_folder: ./UM_dataset/train
    csv_path: ./UM_dataset/train/UM_label.csv

    flip: true
    crop: false
    image_size: !!int 512
    resize: true
    fine_size: 512
    augment_ratio: 1
    # score_scaler: 30
    # mean: [0.485, 0.456, 0.406]
    # std: [0.229, 0.224, 0.225]

    # data loader
    use_shuffle: true
    num_worker_per_gpu: 2
    batch_size_per_gpu: 2
    dataset_enlarge_ratio: 1
    prefetch_mode: ~

  val:
    name: testUMseg
    type: ImagePairDataset
    image_folder: ./UM_dataset/val
    csv_path: ./UM_dataset/val/UM_label.csv

    flip: true
    crop: false
    image_size: !!int 512
    resize: true
    fine_size: 512
    augment_ratio: 1
    # score_scaler: 30
    # mean: [0.485, 0.456, 0.406]
    # std: [0.229, 0.224, 0.225]

# network structures
network_g:
  type: UMSeg
  dim: 32
  out_chan: 1
  stage: 4
  neck_blocks: 8
  layer_scale_init_value: !!float 1e-5
  use_bias: True


network_d:
  type: SwinTransformer
  img_size: 512
  patch_size: 4
  in_chans: 3
  num_classes: 1 #
  embed_dim: 64
  depths: [2, 2, 6, 2]
  num_heads: [4, 8, 16, 32]
  window_size: 16 # windows here
  mlp_ratio: 4.
  qkv_bias: True
  drop_rate: 0.
  attn_drop_rate: 0.
  drop_path_rate: 0.1
  ape: False
  patch_norm: True
  use_checkpoint: False
  sigmoid: False
  

# path
path:
  pretrain_network_g: 
  strict_load_g: true
  resume_state: ~

# training settings
train:
  ema_decay: 0.99
  optim_g:
    type: Adam
    lr: !!float 1e-5
    weight_decay: 0
    betas: [0.9, 0.99]

  optim_d: # aligned with net_d
    type: Adam
    lr: !!float 1e-5
    weight_decay: 0
    betas: [0.9, 0.99]

  scheduler:
    type: MultiStepLR
    milestones: [5000, 10000, 20000, 40000]
    gamma: 0.5

  total_iter: 80000
  warmup_iter: 500  # no warm up use -1

  net_d_iters: 1   # train net_d ever x round
  net_d_init_iters: 1000  # train net_g from x round

  # losses
  pixel_opt:
    type: SmoothL1Loss
    loss_weight: !!float 0.2
    reduction: mean
  # dice_opt:
  #   type: DiceLoss
  #   smooth: 1.0
  #   loss_weight: !!float 1
  grad_opt:
    type: GradLoss
    loss_weight: !!float 0.2
  focal_opt:
    type: FocalLoss
    gamma: 2
    # alpha: None
    loss_weight: !!float 0.1
  net_d_opt: 
    type: SmoothL1Loss
    loss_weight: !!float 1
    reduction: mean


# validation settings
val:
  val_freq:  5000
  suffix: seg
  use_pbar: false
  
  save_img: true
  metrics:
    L1:
      type: calculate_l1
      better: lower
    IOU:
      type: calculate_IOU
      better: higher
    Dice:
      type: calculate_Dice
      better: higher
  
  save_prediction: true
  iqa_metrics:
    SRCC:
      type: calculate_srcc
      better: higher
    PLCC:
      type: calculate_plcc
      better: higher
    RMSE:
      type: calculate_rmse
      better: lower

# logging settings
logger:
  print_freq: 2500
  save_checkpoint_freq: 5000 # save point should be later than validation point 
  save_best: true

# dist training settings
# dist_params:
#   backend: nccl
#   port: 29500